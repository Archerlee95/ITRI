{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMm+RCWDUkiXJvsidL+WIhF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 抓取劇名"],"metadata":{"id":"J1pix8zV-_vX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYmRO5PVhgBc"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import pandas as pd"]},{"cell_type":"code","source":["dfs = []\n","\n","# Loop through the years\n","for i in range(2013, 2024, 1):\n","    url = f\"http://dorama.info/state/ranking_rate.php?year={i}&aa=1&gk=1\"\n","    jp_data = pd.read_html(url)\n","\n","    # Assuming there are at least 21 tables in the HTML, and you want the 20th table\n","    df_jp = jp_data[20]\n","\n","    # Add a new column for the year\n","    df_jp['Year'] = i\n","\n","    # Append the DataFrame to the list\n","    dfs.append(df_jp)\n","\n","# Concatenate all DataFrames in the list into a single DataFrame\n","result_df = pd.concat(dfs, ignore_index=True)"],"metadata":{"id":"t5NnhpWs8_iU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_df"],"metadata":{"id":"y_eEL2uapk3n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_df.to_csv('jp_ranking_data_combined.csv', index=False, encoding='utf-8')"],"metadata":{"id":"Gp6Kf_G6-Htj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 抓取演員 URL 分類"],"metadata":{"id":"2Z3jNDIY-5pQ"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# Create empty lists to store data\n","data_list = []\n","\n","def fetch_urls(year):\n","    url = f\"http://dorama.info/state/ranking_rate.php?year={year}&aa=1&gk=1\"\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","    area1 = soup.find_all('td', {'width': '150', 'class': 'td_dt_g'})\n","    x = 0\n","    urls = []\n","\n","    for i in area1:\n","        url1 = i.find(\"a\")\n","        if url1 is not None:\n","            url2 = 'http://dorama.info' + url1.get('href')\n","            urls.append(url2)\n","            x += 1\n","            if x == 30:\n","                break\n","\n","    print(f\"Year: {year}, Number of URLs: {x}\")\n","    return urls\n","\n","def fetch_categories(url):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","    area1 = soup.find_all('td', {'valign': 'top', 'class': 'td_dt_g'})\n","    categories = []\n","\n","    for i in area1:\n","        category1 = i.find_all(\"a\")\n","        for j in category1:\n","            categories.append(j.text)\n","\n","    return categories\n","\n","def fetch_actor(url):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, \"html.parser\")\n","    area2 = soup.find_all('td', {'width': '236', 'class': 'td_g'})\n","    actor1 = area2[1].text.replace('／', '')\n","\n","    return actor1\n","\n","# Iterate over the years\n","for year in range(2013, 2024, 1):\n","    # Fetch URLs for the current year\n","    urls = fetch_urls(year)\n","\n","    # Fetch categories and actors for each URL\n","    for url in urls:\n","        categories = fetch_categories(url)\n","        actor = fetch_actor(url)\n","\n","        # Store data in a dictionary\n","        data = {\n","            'Year': year,\n","            'URL': url,\n","            'Categories': categories,\n","            'Actor': actor\n","        }\n","\n","        # Append the dictionary to the list\n","        data_list.append(data)\n","\n","# Convert the list of dictionaries to a DataFrame\n","result_df2 = pd.DataFrame(data_list)"],"metadata":{"id":"VC85jB1SnOib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_df2.to_csv('jp_ranking_data_combined2.csv', index=False,  encoding='utf-8')"],"metadata":{"id":"gaYC77dnp8fV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_df2"],"metadata":{"id":"kxIIy4bYo5p7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 抓取KKTV 台灣排行"],"metadata":{"id":"ytg7swWS_D6v"}},{"cell_type":"code","source":["import pandas as pd\n","from bs4 import BeautifulSoup\n","import requests\n","\n","url3 = \"http://dorama.info/drama/d_award.php?awd=32&list_id=449&mode=1\"\n","jp_data3 = pd.read_html(url3)\n","df = jp_data3[20]\n","\n","df.to_csv(\"your_file.csv\", index=False)"],"metadata":{"id":"WDxkpVUl_DWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url4 = \"http://dorama.info/drama/d_award.php?awd=32&list_id=489&mode=1\"\n","jp_data3 = pd.read_html(url4)\n","df = jp_data3[20]\n","\n","df.to_csv(\"your_file1.csv\", index=False)"],"metadata":{"id":"K4rMx_uP_po3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url5 = \"http://dorama.info/drama/d_award.php?awd=32&list_id=527&mode=1\"\n","jp_data3 = pd.read_html(url5)\n","df = jp_data3[20]\n","\n","df.to_csv(\"your_file2.csv\", index=False)"],"metadata":{"id":"HQAoIiHDAHxG"},"execution_count":null,"outputs":[]}]}